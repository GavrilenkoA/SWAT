{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Protein Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install transformers[torch] evaluate datasets requests pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we're going to do some transfer learning to fine-tune some large, pre-trained protein language models on tasks of interest. If that sentence feels a bit intimidating to you, don't panic - there's [a blog post](https://huggingface.co/blog/deep-learning-with-proteins) that explains the concepts here in much more detail.\n",
    "\n",
    "The specific model we're going to use is ESM-2, which is the state-of-the-art protein language model at the time of writing (November 2022). The citation for this model is [Lin et al, 2022](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1).\n",
    "\n",
    "There are several ESM-2 checkpoints with differing model sizes. Larger models will generally have better accuracy, but they require more GPU memory and will take much longer to train. The available ESM-2 checkpoints (at time of writing) are:\n",
    "\n",
    "| Checkpoint name | Num layers | Num parameters |\n",
    "|------------------------------|----|----------|\n",
    "| `esm2_t48_15B_UR50D`         | 48 | 15B     |\n",
    "| `esm2_t36_3B_UR50D`          | 36 | 3B      |\n",
    "| `esm2_t33_650M_UR50D`        | 33 | 650M    |\n",
    "| `esm2_t30_150M_UR50D`        | 30 | 150M    |\n",
    "| `esm2_t12_35M_UR50D`         | 12 | 35M     |\n",
    "| `esm2_t6_8M_UR50D`           | 6  | 8M      |\n",
    "\n",
    "Note that the larger checkpoints may be very difficult to train without a large cloud GPU like an A100 or H100, and the largest 15B parameter checkpoint will probably be impossible to train on **any** single GPU! Also, note that memory usage for attention during training will scale as `O(batch_size * num_layers * seq_len^2)`, so larger models on long sequences will use quite a lot of memory! We will use the `esm2_t12_35M_UR50D` checkpoint for this notebook, which should train on any Colab instance or modern GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/esm2_t33_650M_UR50D\"\n",
    "#model_checkpoint = 'facebook/esm2_t36_3B_UR50D'\n",
    "\n",
    "rep_layers = 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['sequence', 'ss_H']\n",
    "data = data[cols]\n",
    "data.rename(columns={'ss_H': 'label'}, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check to make sure we got it right\n",
    "sequences = data['sequence'].to_list()\n",
    "labels = data['label'].to_list()\n",
    "assert len(sequences) == len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data\n",
    "\n",
    "Since the data we're loading isn't prepared for us as a machine learning dataset, we'll have to split the data into train and test sets ourselves! We can use sklearn's function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(sequences, labels, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the data\n",
    "\n",
    "All inputs to neural nets must be numerical. The process of converting strings into numerical indices suitable for a neural net is called **tokenization**. For natural language this can be quite complex, as usually the network's vocabulary will not contain every possible word, which means the tokenizer must handle splitting rarer words into pieces, as well as all the complexities of capitalization and unicode characters and so on.\n",
    "\n",
    "With proteins, however, things are very easy. In protein language models, each amino acid is converted to a single token. Every model on `transformers` comes with an associated `tokenizer` that handles tokenization for it, and protein language models are no different. Let's get our tokenizer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformer models are downloaded here\n",
    "~/.cache/huggingface/hub/ \n",
    "# from fair-esm\n",
    "~/.cache/torch/hub/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(train_sequences[0], max_length=max_pad, truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input id: 0 = cls, numbers(1-22) according to the amino acid, 1 = pad\n",
    "attention: 1 = actual aa, 0= padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good! We can see that our sequence has been converted into `input_ids`, which is the tokenized sequence, and an `attention_mask`. The attention mask handles the case when we have sequences of variable length - in those cases, the shorter sequences are padded with blank \"padding\" tokens, and the attention mask is padded with 0s to indicate that those tokens should be ignored by the model.\n",
    "\n",
    "So now, let's tokenize our whole dataset. Note that we don't need to do anything with the labels, as they're already in the format we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = tokenizer(train_sequences, max_length=max_pad, truncation=True, padding='max_length')\n",
    "test_tokenized = tokenizer(test_sequences, max_length=max_pad, truncation=True, padding='max_length')\n",
    "test_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation\n",
    "\n",
    "Now we want to turn this data into a dataset that PyTorch can load samples from. We can use the HuggingFace `Dataset` class for this, although if you prefer you can also use `torch.utils.data.Dataset`, at the cost of some more boilerplate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "test_dataset = Dataset.from_dict(test_tokenized)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good, but we're missing our labels! Let's add those on as an extra column to the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
    "test_dataset = test_dataset.add_column(\"labels\", test_labels)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model loading\n",
    "\n",
    "Next, we want to load our model. Make sure to use exactly the same model as you used when loading the tokenizer, or your model might not understand the tokenization scheme you're using!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "#from transformers import AutoModel, TrainingArguments, Trainer\n",
    "\n",
    "#num_labels = max(train_labels + test_labels) + 1  # Add 1 since 0 can be a label\n",
    "num_labels = 1 # regression\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "\n",
    "# multiple GPUs\n",
    "if torch.cuda.is_available():\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    model.cuda()\n",
    "\n",
    "freeze_ = 6 # number of last layers to update, rest will be freeze\n",
    "for i in range(rep_layers-freeze_):\n",
    "    for param in model.module.base_model.encoder.layer[i].parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "#################### here we can freeze layers ####################\n",
    "# Here's an way to freeze layers for a generic transformer model\n",
    "# freeze_ = 6 # number of last layers to update, rest will be freeze\n",
    "# for i in range(rep_layers-freeze_):\n",
    "#     for param in model.base_model.encoder.layer[i].parameters():\n",
    "#         param.requires_grad = False\n",
    "\n",
    "\n",
    "print()\n",
    "frozen_layers = []\n",
    "trainable_layers = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"layer\" in name:  # Only process names containing \"layer\"\n",
    "        if not param.requires_grad:\n",
    "            frozen_layers.append(name)\n",
    "            #print(name, \"False\")\n",
    "        else:\n",
    "            trainable_layers.append(name)\n",
    "            #print(name, \"True\")\n",
    "\n",
    "print(f\"Number of frozen layers: {int(len(frozen_layers)/16)}\")\n",
    "print(f\"Number of trainable layers: {int(len(trainable_layers)/16)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These warnings are telling us that the model is discarding some weights that it used for language modelling (the `lm_head`) and adding some weights for sequence classification (the `classifier`). This is exactly what we expect when we want to fine-tune a language model on a sequence classification task!\n",
    "\n",
    "Next, we initialize our `TrainingArguments`. These control the various training hyperparameters, and will be passed to our `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "lr = 0.00001\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-regression\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=lr,  # Consider experimenting with this based on model performance\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",  # More suitable for regression\n",
    "    push_to_hub=False,\n",
    "    logging_steps=50,  # Adjusted for more frequent logging, modify as needed\n",
    "    # Optional settings based on your dataset and compute resources:\n",
    "    # gradient_accumulation_steps=2,  # Use if effective batch size needs to be larger\n",
    "    # lr_scheduler_type=\"linear\",  # Linear scheduler can be effective with a warmup phase\n",
    "    # warmup_steps=50,  # Number of warmup steps, adjust as needed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**weight_decay** adds a penalty to the loss function based on the magnitude of the weights in the model. This penalty discourages the model from having large weights, which can lead to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the metric we will use to evaluate our models and write a `compute_metrics` function. We can load this from the `evaluate` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     #predictions = predictions.squeeze()  \n",
    "#     predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "#     mse = mean_squared_error(labels, predictions)\n",
    "#     mae = mean_absolute_error(labels, predictions)\n",
    "#     r2 = r2_score(labels, predictions)\n",
    "\n",
    "#     return {\n",
    "#         'mean_squared_error': mse,\n",
    "#         'mean_absolute_error': mae,\n",
    "#         'r2_score': r2\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And at last we're ready to initialize our `Trainer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**early_stopping_patience** evaluating your model after each epoch, then if the validation loss (or another specified metric) doesn't improve for 3 consecutive evaluations (i.e., epochs), the training will be stopped.\n",
    "\n",
    "**early_stopping_threshold** is the improvement in the evaluation metric (e.g., a decrease in validation loss) must be at least 0.001 for the evaluation to be considered \"better.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now finetune our model by just calling the `train` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.read_table('fine_tuning/fine_tunning_esm2/results_finetunning.txt').head(4)\n",
    "sns.lineplot(x=res['Epoch'], y=res['Training Loss'], label='Train eval loss')\n",
    "sns.lineplot(x=res['Epoch'], y=res['Validation Loss'], label='Test eval loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_val = pandas.read_excel('/stor/work/Wilke/luiz/DMS_ML_AMP/data/pg1_muts_validation_set.xlsx', usecols=['ID', 'Sequence', 'MIC MH', '%hemo'])\n",
    "pg_val.replace('>', '', regex=True, inplace=True)\n",
    "pg_val['label'] = [1 if x <= 16 else 0 for x in pg_val['MIC MH'].astype(float)]\n",
    "\n",
    "test_seqs = df_val['Sequence'].to_list()\n",
    "true_labels  = df_val['label'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tok = tokenizer(test_seqs)\n",
    "test_ = Dataset.from_dict(test_tok)\n",
    "test_ = test_.add_column(\"labels\", true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(test_)\n",
    "predicted_classes = np.argmax(predictions[0], axis=1)\n",
    "accuracy = np.mean(predicted_classes == true_labels)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_m = metrics.confusion_matrix(true_labels, predicted_classes)\n",
    "\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in conf_m.flatten()]\n",
    "\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in conf_m.flatten()/np.sum(conf_m)]\n",
    "\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "\n",
    "labelsx = ['Non_active', 'Active']\n",
    "sns.heatmap(conf_m, annot=labels, fmt='', cmap=\"YlGnBu\", yticklabels=labelsx, xticklabels=labelsx)\n",
    "plt.title('Confusion Matrix', fontsize=16)\n",
    "plt.ylabel('Actual label', size=14)\n",
    "plt.xlabel('Predicted label', size=14)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
