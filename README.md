[![author](https://img.shields.io/badge/author-Authors-blue.svg)](https://wilkelab.org) [![](https://img.shields.io/badge/python-3.8+-yellow.svg)](https://www.python.org/downloads/release/python) [![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/allmanbrent/picornavirus_2C_protein/issues) [![GPLv3 license](https://img.shields.io/badge/License-GPLv3-lightgrey.svg)](http://perso.crans.org/besson/LICENSE.html)


# Bigger is not always better in protein language models
<sub>*Data Analysis</sub>


**Description:** Data Science, Bioinformatics, Python, bash-unix.


**Links:**

* [Link](https://wilkelab.org)


# About the project:

## Overview

The trend of increasing size of Protein Language Models (pLMs) necessitates the benchmarking of model performance. While logic may say that bigger = better, biological complexity and dataset size limitations/diversity can complicate analysis. This project offers an in-depth investigation into optimizing transfer learning with Evolutionary Scale Modeling 2 (ESM2), and evaluates the performance of the model's embeddings across different parameter sizes to determine the impact of size on transfer learning in biological datasets. 

### Objective

This project aims to 
* create a deeper understanding on the relationship between model size and maximizing transfer learning
** especially when PLMs follow the trend of LLMs and get bigger and bigger
* is bigger really more effective => takes a lot more computational power and resources that are usually not as available in academia
* find the model size that maximizes transfer learning in different types of biological datasets
* 

### Methodology

text

* Python 
* Pandas 
* Blast 

### Innovation and Impact

text

* 1 - ?
* 2 - ?
* 3 - ?


### References

https://www.who.int/
https://covid.saude.gov.br/


